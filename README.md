# Style-Blending-Using-Neural-Networks
A generative model that uses style transfer to blend the two images to generate new one.

Neural style transfer is a technique used in deep learning to transfer the style of one image to another image while preserving the content of the original image. It works by using a pre-trained neural network to separate and recombine the content and style of two different images.

The process involves three main steps:

1. Feature extraction: The first step is to use a pre-trained convolutional neural network (CNN) (here used VGG19 model) to extract the features of the content and style images. The CNN is typically trained on a large dataset, such as ImageNet, and is capable of identifying the different visual features of an image.

    ![1](https://user-images.githubusercontent.com/108794407/226839931-bb2ded4e-2b1a-4177-b9be-66ad24fca9a5.png)


2. Style transfer: The next step is to use the features extracted from the style image to transfer its style to the content image. This is achieved by minimizing the difference between the feature representations of the style image and the generated image. This is done using an optimization algorithm, such as gradient descent, to find the image that best matches the style of the style image.

    ![2](https://user-images.githubusercontent.com/108794407/226840251-a7c2d701-9a04-46eb-8cbe-7b044f86e182.png)


3. Reconstruction: The final step is to reconstruct the image by combining the content of the original image with the transferred style. This is done by passing the generated image through the CNN again and adjusting the weights to preserve the content of the original image.

# Computing Loss
The loss function used in neural style transfer is a combination of two components: a content loss and a style loss.

1. Content Loss: The content loss measures the difference between the features of the generated image and the features of the content image. The content loss is calculated by comparing the feature maps generated by a pre-trained CNN for the content image and the generated image. The goal is to minimize this difference so that the generated image closely resembles the content image.

2. Style Loss: The style loss measures the difference between the features of the generated image and the features of the style image. The style loss is calculated by comparing the Gram matrices of the feature maps generated by the pre-trained CNN for the style image and the generated image. The Gram matrix describes the correlations between different feature maps and represents the texture of the image. The goal is to minimize this difference so that the generated image adopts the style of the style image.

The total loss function is a weighted sum of the content loss and the style loss. The weights are determined by hyperparameters that can be adjusted to control the balance between the content and style of the generated image.

Overall, neural style transfer is a powerful technique that can be used for a range of creative applications, such as generating new artwork or transforming photographs into different styles.

# INSTRUCTIONS
The code is self explainatory. Just add the paths of content image and style image and run the cells step by step.

TUNING HYPERPARAMETRS IS THE KEY TO ACHIEVE THE DESIRED OUTPUT like-
1. No. of iterations
2. Values of a and b, these are the weights of content loss and style loss adjust them to control the balance between content and style image.

# RESULTS
**Content image** and **Style image**  

![3](https://user-images.githubusercontent.com/108794407/226840568-0069e571-41fd-45f8-884e-2100af182639.png)            +      ![4](https://user-images.githubusercontent.com/108794407/226841180-b900ed18-b797-4f6f-a5c8-3bcd586cf8ed.jpg) 

**Generated Image**

![5](https://user-images.githubusercontent.com/108794407/226842951-dd7ded87-a03c-43f6-9d4e-84520aad02c8.jpg)




